{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G:\\GeekDS\\venv\\python.exe\n",
      "G:\\GeekDS\\venv\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)\n",
    "print(sys.prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in g:\\geekds\\venv\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: click in g:\\geekds\\venv\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in g:\\geekds\\venv\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in g:\\geekds\\venv\\lib\\site-packages (from nltk) (2024.7.24)\n",
      "Requirement already satisfied: tqdm in g:\\geekds\\venv\\lib\\site-packages (from nltk) (4.66.4)\n",
      "Requirement already satisfied: colorama in g:\\geekds\\venv\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ARLSTem', 'ARLSTem2', 'AbstractLazySequence', 'AffixTagger', 'AlignedSent', 'Alignment', 'AnnotationTask', 'ApplicationExpression', 'Assignment', 'BigramAssocMeasures', 'BigramCollocationFinder', 'BigramTagger', 'BinaryMaxentFeatureEncoding', 'BlanklineTokenizer', 'BllipParser', 'BottomUpChartParser', 'BottomUpLeftCornerChartParser', 'BottomUpProbabilisticChartParser', 'Boxer', 'BrillTagger', 'BrillTaggerTrainer', 'CFG', 'CRFTagger', 'CfgReadingCommand', 'ChartParser', 'ChunkParserI', 'ChunkScore', 'Cistem', 'ClassifierBasedPOSTagger', 'ClassifierBasedTagger', 'ClassifierI', 'ConcordanceIndex', 'ConditionalExponentialClassifier', 'ConditionalFreqDist', 'ConditionalProbDist', 'ConditionalProbDistI', 'ConfusionMatrix', 'ContextIndex', 'ContextTagger', 'ContingencyMeasures', 'CoreNLPDependencyParser', 'CoreNLPParser', 'Counter', 'CrossValidationProbDist', 'DRS', 'DecisionTreeClassifier', 'DefaultTagger', 'DependencyEvaluator', 'DependencyGrammar', 'DependencyGraph', 'DependencyProduction', 'DictionaryConditionalProbDist', 'DictionaryProbDist', 'DiscourseTester', 'DrtExpression', 'DrtGlueReadingCommand', 'ELEProbDist', 'EarleyChartParser', 'Expression', 'FStructure', 'FeatDict', 'FeatList', 'FeatStruct', 'FeatStructReader', 'Feature', 'FeatureBottomUpChartParser', 'FeatureBottomUpLeftCornerChartParser', 'FeatureChartParser', 'FeatureEarleyChartParser', 'FeatureIncrementalBottomUpChartParser', 'FeatureIncrementalBottomUpLeftCornerChartParser', 'FeatureIncrementalChartParser', 'FeatureIncrementalTopDownChartParser', 'FeatureTopDownChartParser', 'FreqDist', 'HTTPPasswordMgrWithDefaultRealm', 'HeldoutProbDist', 'HiddenMarkovModelTagger', 'HiddenMarkovModelTrainer', 'HunposTagger', 'IBMModel', 'IBMModel1', 'IBMModel2', 'IBMModel3', 'IBMModel4', 'IBMModel5', 'ISRIStemmer', 'ImmutableMultiParentedTree', 'ImmutableParentedTree', 'ImmutableProbabilisticMixIn', 'ImmutableProbabilisticTree', 'ImmutableTree', 'IncrementalBottomUpChartParser', 'IncrementalBottomUpLeftCornerChartParser', 'IncrementalChartParser', 'IncrementalLeftCornerChartParser', 'IncrementalTopDownChartParser', 'Index', 'InsideChartParser', 'JSONTaggedDecoder', 'JSONTaggedEncoder', 'KneserNeyProbDist', 'LancasterStemmer', 'LaplaceProbDist', 'LazyConcatenation', 'LazyEnumerate', 'LazyIteratorList', 'LazyMap', 'LazySubsequence', 'LazyZip', 'LeftCornerChartParser', 'LegalitySyllableTokenizer', 'LidstoneProbDist', 'LineTokenizer', 'LogicalExpressionException', 'LongestChartParser', 'MLEProbDist', 'MWETokenizer', 'Mace', 'MaceCommand', 'MaltParser', 'MaxentClassifier', 'Model', 'MultiClassifierI', 'MultiParentedTree', 'MutableProbDist', 'NLTKWordTokenizer', 'NaiveBayesClassifier', 'NaiveBayesDependencyScorer', 'NgramAssocMeasures', 'NgramTagger', 'NonprojectiveDependencyParser', 'Nonterminal', 'OrderedDict', 'PCFG', 'Paice', 'ParallelProverBuilder', 'ParallelProverBuilderCommand', 'ParentedTree', 'ParserI', 'PerceptronTagger', 'PhraseTable', 'PorterStemmer', 'PositiveNaiveBayesClassifier', 'ProbDistI', 'ProbabilisticDependencyGrammar', 'ProbabilisticMixIn', 'ProbabilisticNonprojectiveParser', 'ProbabilisticProduction', 'ProbabilisticProjectiveDependencyParser', 'ProbabilisticTree', 'Production', 'ProjectiveDependencyParser', 'Prover9', 'Prover9Command', 'ProxyBasicAuthHandler', 'ProxyDigestAuthHandler', 'ProxyHandler', 'PunktSentenceTokenizer', 'QuadgramAssocMeasures', 'QuadgramCollocationFinder', 'RSLPStemmer', 'RTEFeatureExtractor', 'RUS_PICKLE', 'RandomChartParser', 'RangeFeature', 'ReadingCommand', 'RecursiveDescentParser', 'RegexpChunkParser', 'RegexpParser', 'RegexpStemmer', 'RegexpTagger', 'RegexpTokenizer', 'ReppTokenizer', 'ResolutionProver', 'ResolutionProverCommand', 'SExprTokenizer', 'SLASH', 'Senna', 'SennaChunkTagger', 'SennaNERTagger', 'SennaTagger', 'SequentialBackoffTagger', 'ShiftReduceParser', 'SimpleGoodTuringProbDist', 'SklearnClassifier', 'SlashFeature', 'SnowballStemmer', 'SpaceTokenizer', 'StackDecoder', 'StanfordNERTagger', 'StanfordPOSTagger', 'StanfordSegmenter', 'StanfordTagger', 'StemmerI', 'SteppingChartParser', 'SteppingRecursiveDescentParser', 'SteppingShiftReduceParser', 'SyllableTokenizer', 'TYPE', 'TabTokenizer', 'TableauProver', 'TableauProverCommand', 'TaggerI', 'TestGrammar', 'Text', 'TextCat', 'TextCollection', 'TextTilingTokenizer', 'TnT', 'TokenSearcher', 'ToktokTokenizer', 'TopDownChartParser', 'TransitionParser', 'Tree', 'TreePrettyPrinter', 'TreebankWordDetokenizer', 'TreebankWordTokenizer', 'Trie', 'TrigramAssocMeasures', 'TrigramCollocationFinder', 'TrigramTagger', 'TweetTokenizer', 'TypedMaxentFeatureEncoding', 'Undefined', 'UniformProbDist', 'UnigramTagger', 'UnsortedChartParser', 'Valuation', 'Variable', 'ViterbiParser', 'WekaClassifier', 'WhitespaceTokenizer', 'WittenBellProbDist', 'WordNetLemmatizer', 'WordPunctTokenizer', '__author__', '__author_email__', '__builtins__', '__cached__', '__classifiers__', '__copyright__', '__doc__', '__file__', '__keywords__', '__license__', '__loader__', '__longdescr__', '__maintainer__', '__maintainer_email__', '__name__', '__package__', '__path__', '__spec__', '__url__', '__version__', 'accuracy', 'acyclic_branches_depth_first', 'acyclic_breadth_first', 'acyclic_depth_first', 'acyclic_dic2tree', 'add_logs', 'agreement', 'align', 'alignment_error_rate', 'aline', 'api', 'app', 'apply_features', 'approxrand', 'arity', 'arlstem', 'arlstem2', 'association', 'bigrams', 'binary_distance', 'binary_search_file', 'binding_ops', 'bisect', 'blankline_tokenize', 'bleu', 'bleu_score', 'bllip', 'boolean_ops', 'boxer', 'bracket_parse', 'breadth_first', 'brill', 'brill_trainer', 'build_opener', 'call_megam', 'casual', 'casual_tokenize', 'ccg', 'chain', 'chart', 'chat', 'chomsky_normal_form', 'choose', 'chrf', 'chrf_score', 'chunk', 'cistem', 'classify', 'clause', 'clean_html', 'clean_url', 'cluster', 'collapse_unary', 'collections', 'collocations', 'combinations', 'compat', 'config_java', 'config_megam', 'config_weka', 'conflicts', 'confusionmatrix', 'conllstr2tree', 'conlltags2tree', 'corenlp', 'corpus', 'crf', 'custom_distance', 'data', 'decisiontree', 'decorator', 'decorators', 'defaultdict', 'demo', 'dependencygraph', 'deprecated', 'deque', 'destructive', 'discourse', 'distance', 'download', 'download_gui', 'download_shell', 'downloader', 'draw', 'drt', 'earleychart', 'edge_closure', 'edges2dot', 'edit_distance', 'edit_distance_align', 'elementtree_indent', 'entropy', 'equality_preds', 'evaluate', 'evaluate_sents', 'everygrams', 'extract', 'extract_rels', 'extract_test_sentences', 'f_measure', 'featstruct', 'featurechart', 'filestring', 'find', 'flatten', 'fractional_presence', 'gale_church', 'gdfa', 'getproxies', 'ghd', 'gleu', 'gleu_score', 'glue', 'grammar', 'grow_diag_final_and', 'guess_encoding', 'help', 'hmm', 'hunpos', 'ibm1', 'ibm2', 'ibm3', 'ibm4', 'ibm5', 'ibm_model', 'ieerstr2tree', 'in_idle', 'induce_pcfg', 'inference', 'infile', 'inspect', 'install_opener', 'internals', 'interpret_sents', 'interval_distance', 'invert_dict', 'invert_graph', 'is_rel', 'islice', 'isri', 'jaccard_distance', 'json_tags', 'jsontags', 'lancaster', 'lazyimport', 'legality_principle', 'lfg', 'line_tokenize', 'linearlogic', 'lm', 'load', 'load_parser', 'locale', 'log_likelihood', 'logic', 'mace', 'malt', 'map_tag', 'mapping', 'masi_distance', 'maxent', 'megam', 'memoize', 'meteor', 'meteor_score', 'metrics', 'misc', 'mwe', 'naivebayes', 'ne_chunk', 'ne_chunk_sents', 'ngrams', 'nist', 'nist_score', 'nonprojectivedependencyparser', 'nonterminals', 'numpy', 'os', 'pad_sequence', 'paice', 'pairwise', 'parallelize_preprocess', 'parse', 'parse_sents', 'pchart', 'perceptron', 'phrase_based', 'pk', 'porter', 'pos_tag', 'pos_tag_sents', 'positivenaivebayes', 'pprint', 'pr', 'precision', 'presence', 'print_string', 'probability', 'projectivedependencyparser', 'prover9', 'punkt', 'pydoc', 'raise_unorderable_types', 'ranks_from_scores', 'ranks_from_sequence', 're', 're_show', 'read_grammar', 'read_logic', 'read_valuation', 'recall', 'recursivedescent', 'regexp', 'regexp_span_tokenize', 'regexp_tokenize', 'register_tag', 'relextract', 'repp', 'resolution', 'ribes', 'ribes_score', 'root_semrep', 'rslp', 'rte_classifier', 'rte_classify', 'rte_features', 'rtuple', 'scikitlearn', 'scores', 'segmentation', 'sem', 'senna', 'sent_tokenize', 'sequential', 'set2rel', 'set_proxy', 'sexpr', 'sexpr_tokenize', 'shiftreduce', 'simple', 'sinica_parse', 'skipgrams', 'skolemize', 'slice_bounds', 'snowball', 'sonority_sequencing', 'spearman', 'spearman_correlation', 'stack_decoder', 'stanford', 'stanford_segmenter', 'stem', 'str2tuple', 'string_span_tokenize', 'subprocess', 'subsumes', 'sum_logs', 'tableau', 'tadm', 'tag', 'tagset_mapping', 'tagstr2tree', 'tbl', 'tee', 'text', 'textcat', 'texttiling', 'textwrap', 'tkinter', 'tnt', 'tokenize', 'tokenwrap', 'toktok', 'toolbox', 'total_ordering', 'trace', 'transitionparser', 'transitive_closure', 'translate', 'tree', 'tree2conllstr', 'tree2conlltags', 'treebank', 'trigrams', 'tuple2str', 'un_chomsky_normal_form', 'unify', 'unique_list', 'untag', 'unweighted_minimum_spanning_dict', 'unweighted_minimum_spanning_digraph', 'unweighted_minimum_spanning_tree', 'usage', 'util', 'version_file', 'viterbi', 'warnings', 'weka', 'windowdiff', 'word_tokenize', 'wordnet', 'wordpunct_tokenize', 'wsd']\n"
     ]
    }
   ],
   "source": [
    "print(dir(nltk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G:\\GeekDS\\venv\\lib\\site-packages\\nltk\\__init__.py\n"
     ]
    }
   ],
   "source": [
    "print(nltk.__file__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "nltk_dir = os.path.dirname(nltk.__file__)\n",
    "nltk_content = os.listdir(nltk_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['app', 'book.py', 'ccg', 'chat', 'chunk', 'classify', 'cli.py', 'cluster', 'collections.py', 'collocations.py', 'compat.py', 'corpus', 'data.py', 'decorators.py', 'downloader.py', 'draw', 'featstruct.py', 'grammar.py', 'help.py', 'inference', 'internals.py', 'jsontags.py', 'langnames.py', 'lazyimport.py', 'lm', 'metrics', 'misc', 'parse', 'probability.py', 'sem', 'sentiment', 'stem', 'tag', 'tbl', 'test', 'text.py', 'tgrep.py', 'tokenize', 'toolbox.py', 'translate', 'tree', 'treeprettyprinter.py', 'treetransforms.py', 'twitter', 'util.py', 'VERSION', 'wsd.py', '__init__.py', '__pycache__']\n"
     ]
    }
   ],
   "source": [
    "print(nltk_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Natural Language Toolkit: Tokenizers\n",
      "#\n",
      "# Copyright (C) 2001-2023 NLTK Project\n",
      "# Author: Edward Loper <edloper@gmail.com>\n",
      "#         Steven Bird <stevenbird1@gmail.com> (minor additions)\n",
      "# Contributors: matthewmc, clouds56\n",
      "# URL: <https://www.nltk.org/>\n",
      "# For license information, see LICENSE.TXT\n",
      "\n",
      "r\"\"\"\n",
      "NLTK Tokenizer Package\n",
      "\n",
      "Tokenizers divide strings into lists of substrings.  For example,\n",
      "tokenizers can be used to find the words and punctuation in a string:\n",
      "\n",
      "    >>> from nltk.tokenize import word_tokenize\n",
      "    >>> s = '''Good muffins cost $3.88\\nin New York.  Please buy me\n",
      "    ... two of them.\\n\\nThanks.'''\n",
      "    >>> word_tokenize(s) # doctest: +NORMALIZE_WHITESPACE\n",
      "    ['Good', 'muffins', 'cost', '$', '3.88', 'in', 'New', 'York', '.',\n",
      "    'Please', 'buy', 'me', 'two', 'of', 'them', '.', 'Thanks', '.']\n",
      "\n",
      "This particular tokenizer requires the Punkt sentence tokenization\n",
      "models to be installed. NLTK also provides a simpler,\n",
      "regular-expression based tokenizer, which splits text on whitespace\n",
      "and punctuation:\n",
      "\n",
      "    >>> from nltk.tokenize import wordpunct_tokenize\n",
      "    >>> wordpunct_tokenize(s) # doctest: +NORMALIZE_WHITESPACE\n",
      "    ['Good', 'muffins', 'cost', '$', '3', '.', '88', 'in', 'New', 'York', '.',\n",
      "    'Please', 'buy', 'me', 'two', 'of', 'them', '.', 'Thanks', '.']\n",
      "\n",
      "We can also operate at the level of sentences, using the sentence\n",
      "tokenizer directly as follows:\n",
      "\n",
      "    >>> from nltk.tokenize import sent_tokenize, word_tokenize\n",
      "    >>> sent_tokenize(s)\n",
      "    ['Good muffins cost $3.88\\nin New York.', 'Please buy me\\ntwo of them.', 'Thanks.']\n",
      "    >>> [word_tokenize(t) for t in sent_tokenize(s)] # doctest: +NORMALIZE_WHITESPACE\n",
      "    [['Good', 'muffins', 'cost', '$', '3.88', 'in', 'New', 'York', '.'],\n",
      "    ['Please', 'buy', 'me', 'two', 'of', 'them', '.'], ['Thanks', '.']]\n",
      "\n",
      "Caution: when tokenizing a Unicode string, make sure you are not\n",
      "using an encoded version of the string (it may be necessary to\n",
      "decode it first, e.g. with ``s.decode(\"utf8\")``.\n",
      "\n",
      "NLTK tokenizers can produce token-spans, represented as tuples of integers\n",
      "having the same semantics as string slices, to support efficient comparison\n",
      "of tokenizers.  (These methods are implemented as generators.)\n",
      "\n",
      "    >>> from nltk.tokenize import WhitespaceTokenizer\n",
      "    >>> list(WhitespaceTokenizer().span_tokenize(s)) # doctest: +NORMALIZE_WHITESPACE\n",
      "    [(0, 4), (5, 12), (13, 17), (18, 23), (24, 26), (27, 30), (31, 36), (38, 44),\n",
      "    (45, 48), (49, 51), (52, 55), (56, 58), (59, 64), (66, 73)]\n",
      "\n",
      "There are numerous ways to tokenize text.  If you need more control over\n",
      "tokenization, see the other methods provided in this package.\n",
      "\n",
      "For further information, please see Chapter 3 of the NLTK book.\n",
      "\"\"\"\n",
      "\n",
      "import re\n",
      "\n",
      "from nltk.data import load\n",
      "from nltk.tokenize.casual import TweetTokenizer, casual_tokenize\n",
      "from nltk.tokenize.destructive import NLTKWordTokenizer\n",
      "from nltk.tokenize.legality_principle import LegalitySyllableTokenizer\n",
      "from nltk.tokenize.mwe import MWETokenizer\n",
      "from nltk.tokenize.punkt import PunktSentenceTokenizer\n",
      "from nltk.tokenize.regexp import (\n",
      "    BlanklineTokenizer,\n",
      "    RegexpTokenizer,\n",
      "    WhitespaceTokenizer,\n",
      "    WordPunctTokenizer,\n",
      "    blankline_tokenize,\n",
      "    regexp_tokenize,\n",
      "    wordpunct_tokenize,\n",
      ")\n",
      "from nltk.tokenize.repp import ReppTokenizer\n",
      "from nltk.tokenize.sexpr import SExprTokenizer, sexpr_tokenize\n",
      "from nltk.tokenize.simple import (\n",
      "    LineTokenizer,\n",
      "    SpaceTokenizer,\n",
      "    TabTokenizer,\n",
      "    line_tokenize,\n",
      ")\n",
      "from nltk.tokenize.sonority_sequencing import SyllableTokenizer\n",
      "from nltk.tokenize.stanford_segmenter import StanfordSegmenter\n",
      "from nltk.tokenize.texttiling import TextTilingTokenizer\n",
      "from nltk.tokenize.toktok import ToktokTokenizer\n",
      "from nltk.tokenize.treebank import TreebankWordDetokenizer, TreebankWordTokenizer\n",
      "from nltk.tokenize.util import regexp_span_tokenize, string_span_tokenize\n",
      "\n",
      "\n",
      "# Standard sentence tokenizer.\n",
      "def sent_tokenize(text, language=\"english\"):\n",
      "    \"\"\"\n",
      "    Return a sentence-tokenized copy of *text*,\n",
      "    using NLTK's recommended sentence tokenizer\n",
      "    (currently :class:`.PunktSentenceTokenizer`\n",
      "    for the specified language).\n",
      "\n",
      "    :param text: text to split into sentences\n",
      "    :param language: the model name in the Punkt corpus\n",
      "    \"\"\"\n",
      "    tokenizer = load(f\"tokenizers/punkt/{language}.pickle\")\n",
      "    return tokenizer.tokenize(text)\n",
      "\n",
      "\n",
      "# Standard word tokenizer.\n",
      "_treebank_word_tokenizer = NLTKWordTokenizer()\n",
      "\n",
      "\n",
      "def word_tokenize(text, language=\"english\", preserve_line=False):\n",
      "    \"\"\"\n",
      "    Return a tokenized copy of *text*,\n",
      "    using NLTK's recommended word tokenizer\n",
      "    (currently an improved :class:`.TreebankWordTokenizer`\n",
      "    along with :class:`.PunktSentenceTokenizer`\n",
      "    for the specified language).\n",
      "\n",
      "    :param text: text to split into words\n",
      "    :type text: str\n",
      "    :param language: the model name in the Punkt corpus\n",
      "    :type language: str\n",
      "    :param preserve_line: A flag to decide whether to sentence tokenize the text or not.\n",
      "    :type preserve_line: bool\n",
      "    \"\"\"\n",
      "    sentences = [text] if preserve_line else sent_tokenize(text, language)\n",
      "    return [\n",
      "        token for sent in sentences for token in _treebank_word_tokenizer.tokenize(sent)\n",
      "    ]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import os\n",
    "\n",
    "# Get the directory of the NLTK module\n",
    "nltk_dir = os.path.dirname(nltk.__file__)\n",
    "\n",
    "# Locate the path of the tokenize module\n",
    "tokenize_path = os.path.join(nltk_dir, 'tokenize', '__init__.py')\n",
    "\n",
    "# Open and read the tokenize module file\n",
    "with open(tokenize_path, 'r') as file:\n",
    "    content = file.read()\n",
    "\n",
    "# Print the content of the tokenize module file\n",
    "print(content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = \"\"\"Hello,My name is Lavish Gangwani.\n",
    "I'm born and brought up in Kanpur U.P.\n",
    "Currently I'm Pursuing Course Advance Data Science for AI and ML From Learnbay.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello,My name is Lavish Gangwani.\\nI'm born and brought up in Kanpur U.P.\\nCurrently I'm Pursuing Course Advance Data Science for AI and ML From Learnbay.\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello,My name is Lavish Gangwani.\n",
      "I'm born and brought up in Kanpur U.P.\n",
      "Currently I'm Pursuing Course Advance Data Science for AI and ML From Learnbay.\n"
     ]
    }
   ],
   "source": [
    "print(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting paragraphs into sentences, sentences into words , words into char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = sent_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello,My name is Lavish Gangwani.\n",
      "I'm born and brought up in Kanpur U.P.\n",
      "Currently I'm Pursuing Course Advance Data Science for AI and ML From Learnbay.\n"
     ]
    }
   ],
   "source": [
    "for word in doc:\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " ',',\n",
       " 'My',\n",
       " 'name',\n",
       " 'is',\n",
       " 'Lavish',\n",
       " 'Gangwani',\n",
       " '.',\n",
       " 'I',\n",
       " \"'m\",\n",
       " 'born',\n",
       " 'and',\n",
       " 'brought',\n",
       " 'up',\n",
       " 'in',\n",
       " 'Kanpur',\n",
       " 'U.P',\n",
       " '.',\n",
       " 'Currently',\n",
       " 'I',\n",
       " \"'m\",\n",
       " 'Pursuing',\n",
       " 'Course',\n",
       " 'Advance',\n",
       " 'Data',\n",
       " 'Science',\n",
       " 'for',\n",
       " 'AI',\n",
       " 'and',\n",
       " 'ML',\n",
       " 'From',\n",
       " 'Learnbay',\n",
       " '.']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'My', 'name', 'is', 'Lavish', 'Gangwani', '.']\n",
      "['I', \"'m\", 'born', 'and', 'brought', 'up', 'in', 'Kanpur', 'U.P', '.']\n",
      "['Currently', 'I', \"'m\", 'Pursuing', 'Course', 'Advance', 'Data', 'Science', 'for', 'AI', 'and', 'ML', 'From', 'Learnbay', '.']\n"
     ]
    }
   ],
   "source": [
    "for sent in doc:\n",
    "    print(word_tokenize(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " ',',\n",
       " 'My',\n",
       " 'name',\n",
       " 'is',\n",
       " 'Lavish',\n",
       " 'Gangwani',\n",
       " '.',\n",
       " 'I',\n",
       " \"'\",\n",
       " 'm',\n",
       " 'born',\n",
       " 'and',\n",
       " 'brought',\n",
       " 'up',\n",
       " 'in',\n",
       " 'Kanpur',\n",
       " 'U',\n",
       " '.',\n",
       " 'P',\n",
       " '.',\n",
       " 'Currently',\n",
       " 'I',\n",
       " \"'\",\n",
       " 'm',\n",
       " 'Pursuing',\n",
       " 'Course',\n",
       " 'Advance',\n",
       " 'Data',\n",
       " 'Science',\n",
       " 'for',\n",
       " 'AI',\n",
       " 'and',\n",
       " 'ML',\n",
       " 'From',\n",
       " 'Learnbay',\n",
       " '.']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordpunct_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'My', 'name', 'is', 'Lavish', 'Gangwani', '.']\n",
      "['I', \"'\", 'm', 'born', 'and', 'brought', 'up', 'in', 'Kanpur', 'U', '.', 'P', '.']\n",
      "['Currently', 'I', \"'\", 'm', 'Pursuing', 'Course', 'Advance', 'Data', 'Science', 'for', 'AI', 'and', 'ML', 'From', 'Learnbay', '.']\n"
     ]
    }
   ],
   "source": [
    "for i in doc:\n",
    "    print(wordpunct_tokenize(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
